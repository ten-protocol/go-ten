# The soft-finality guarantee 

The decentralised POBI protocol links the rollups to the L1 block used as proof, which means that any L1 reorg
results in an implicit L2 reorg. The finality of an L2 rollup is fully linked to the finality of an L1 block.

During bootstrapping, we want a central sequencer to guarantee finality in around ~1 second and for the bridge to be decentralised 
and resistant to reorgs.

Note: See bridge and finality designs for more details


## Problem and Requirements

The central sequencer is mainly responsible for the ordering (sequencing) of the L2 transactions.
The ordering of the L1 transactions like the deposits into the TEN bridge, is decided by the Ethereum staking nodes, 
governed by the PoS Consensus.

Given that deposits affect the balances of accounts, which can be used in TEN transactions, there is a dependence 
between the ordering of the L1 messages and the result of executing L2 transactions.

In the "fast finality" design, the "soft finality" of an L2 transaction is defined as the moment when the sequencer produces
a Light Batch(LB) containing that transaction, and that LB is distributed to the network to respond to user requests.

We can see, that there is a tension between the guarantees of "soft-finality" and the re-org resistance of the bridge.

In this document we'll add some nuance to the soft-finality guarantee.

The ideal outcome is that executing a transaction can never result in a different outcome, independent on the L1.

A more realistic outcome is to relax the guarantee such that the tx execution result might be different only under certain
factors unlikely to happen unless there is a re-org attack.


## Outcomes of the design

- In the context of fast finality and bridge designs, define the "Soft finality" guarantee.
- Define the challenge conditions for the sequencer generating competing light batches.
- Based on the above define the structure of a Light batch.

## Assumption

Messages from the L1 are processed as they are found in the L1 blocks, and synthetic transactions are generated based on them.
These synthetic transactions are implicitly included in the light batches.

A "synthetic transaction" is a transaction generated by the TEN protocol from some external data, with the intent of 
bringing that data on the TEN chain, since these transactions will be addressed to a "system contract".
Every TEN node, given the same input data, will generate the same synthetic transactions, and thus correctly verify 
the state root of each rollup.


## Solutions

Since the problem is expressed in terms of "tension", we'll first try to explore the different options, and
understand where they sit on the tradeoff spectrum.


### Option 1 - Adding delays

The straightforward solution is to wait a while before processing deposits so that these transactions are "final" on the L1.
Eventually, the cost of mounting a re-org attack on the L1 becomes prohibitively high, ensuring security.

There are two main disadvantages.

The first is that users must wait before bridge messages are processed, which is not a great UX and it disables certain applications. 

The second is that the security of the funds on the bridge depends on the delay. If the delay is x Ethereum blocks, then 
an attacker must wait for x blocks until the deposit executes on Ten, and then perform a revert attack on Ethereum.
Note that with PoS, if the delay is 64 blocks (2 epochs), then re-org is mostly a theoretical possibility, because in practice
it is prohibitively expensive for anyone. 

The subtlety of this approach is that it can transform TEN into a side-chain even though we roll up to Ethereum.

This depends on the definition of an L2, which is a moving target. If the consensus is that an L2 needs a re-org resistant
bridge, then this option will break it.

On the tradeoff spectrum, this solution lies towards the ideal: "The result of executing this transaction can never result in a different outcome" end,
which is good, but it has some severe disadvantages.

The Sequencer is never allowed to produce a competing sibling Light Batches. As per the fast finality design document, 
any L2 node in possession of multiple LBs with the same "Number", will be able to claim some reward paid from the 
stake of the Sequencer.

### Option 2 - Link light batches to blocks

Like POBI, each LB is generated from a single L1 block and links to it. (Actually, multiple LBs will be linked to a single block)

The sequencer monitors the L1, and the moment a reorg is happening, it will produce new LBs linked to the blocks from 
the new branch, which will then be distributed.

The difference from POBI is that LBs are not published to a management contract in the L1, so they are no longer rejected 
by a "source of truth" authority the moment they are no longer linked to a block found on the canonical chain. 
They are only distributed from the sequencer to the other L2 nodes.
This can lead to chaotic results and hard-to-enforce behaviour. Basically, the sequencer could use the re-org reason to generate competing LBs,
and attempt some MEV, without being punished.

From a UX pov, the soft finality guarantee is not great. If there is an L1 re-org, then the result of a transaction that
has received a response already could change.


### Option 3 - Link light batches to blocks, but guarantee the L2 ordering

This option improves on the previous one by reducing the impact of the L1 reorgs which have no logical relationship 
whatsoever with the L2 transactions.

The critical insight is that the sequencer *can* offer guarantees over the L2 transactions' order, and can be punished if
it creates batches that contain L2 transactions with a different ordering.
Controlling this can be achieved if the sequencer publishes a field containing the Merkle root of a list of transactions.
If two (or more) batches have a different value on that field, the sequencer will be slashed.

The rule is that a sequencer will be able to generate sibling LBs in case an L1 fork is happening, but only as long as the 
order of the L2 transactions is preserved.

```go
type LightBatchHeader struct{
	ParentLB        Hash    // the id of the parent Light Batch
	L1BlockHash     Hash    // the head l1 block at the time of this LB
	L2TxsHash       Hash    // the MTree root of the list of transactions
	Number          int     // the height of this LB - since when? the latest L1 block, the latest rollup, the beginning?
	StateRoot       Hash    // the state root hash
	R, S            *big.Int // signature of enclave
}
```

Note: There can be sibling LBs (same `Number` and `ParentLB`) with the same `L2TxsHash` but different `L1BlockHash`.
If the `L2TxsHash` field is different the sequencer is slashed.

This option offers a much stronger finality guarantee. An L2 transaction can never be replaced with a competing one, 
but it might result in a different outcome only in the case a deposit L1 transaction was processed or not before it.

In practice, we estimate the sibling LBs will result in the same execution result for the vast majority of L2 transactions. 
If a user is not actively trying to double spend a deposit on the L1, then the odds that an L1 re-org have an impact
on the result of executing their transaction are minimal. 


### Option 4 - Guarantee the L2 ordering, but link batches to deposit messages

Another possible improvement is to reduce the reasons for creating sibling LBs to only the situation where
there is an actual change in the ordering of L1 messages (deposits).

The insight is that instead of linking the batch to an L1 block, we can link a batch to a list of L1 messages.
If the order of L1 messages is preserved during an L1 reorg ( which is very likely unless the sender actively
tries an attack), then there is no reason for generating a sibling LB.

```go
type LightBatchHeader struct{
	ParentLB        Hash    // the id of the parent Light Batch
	L1Number        int     // the height of the l1 block used 
	L1MsgsHash      Hash    // the list of messages from the L1.
	L2TxsHash       Hash    // the MTree root of the list of transactions
	Number          int     // the height of this LB - since when? the latest L1 block, the latest rollup, the beginning?
	StateRoot       Hash    // the state root hash
	R, S            *big.Int // signature of enclave
}
```

The drawback of this solution is some logical complexity. As the validators process blocks and batches, they might reach different L1MsgsHash, which needs handling.

Same as in Option 3, an L2 transaction can never be replaced with a competing one, but it might result in a different outcome only if there is a deliberate
double-spend attack on the L1 by the same account.  

## Conclusion

The last two options provide the best UX.
The sequencer guarantees that it will consistently execute L2 transactions in the same order and batches, even if there are reorgs on the L1.
If there is an L1 re-org, but there are no L2 deposits, or if the L2 deposits are included in the same order, then the finality guarantee is complete.

The only case where finality can change is when there is a deliberate double-spend attack on the L1, and this will only affect
the user who is directly involved in that attack.

It is possible in practice to start with option 3 for simplicity, and transition to option 4 as an optimisation. 
This will require a full upgrade with a transition period.


TODO - Reason about sealing an LB chain in a rollup and reorgs


